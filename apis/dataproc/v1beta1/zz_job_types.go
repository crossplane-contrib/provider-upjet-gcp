// SPDX-FileCopyrightText: 2023 The Crossplane Authors <https://crossplane.io>
//
// SPDX-License-Identifier: Apache-2.0

/*
Copyright 2021 The Crossplane Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by upjet. DO NOT EDIT.

package v1beta1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	v1 "github.com/crossplane/crossplane-runtime/apis/common/v1"
)

type HadoopConfigInitParameters struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []LoggingConfigInitParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
	MainClass *string `json:"mainClass,omitempty" tf:"main_class,omitempty"`

	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
	MainJarFileURI *string `json:"mainJarFileUri,omitempty" tf:"main_jar_file_uri,omitempty"`

	// A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`
}

type HadoopConfigObservation struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []LoggingConfigObservation `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
	MainClass *string `json:"mainClass,omitempty" tf:"main_class,omitempty"`

	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
	MainJarFileURI *string `json:"mainJarFileUri,omitempty" tf:"main_jar_file_uri,omitempty"`

	// A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`
}

type HadoopConfigParameters struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	// +kubebuilder:validation:Optional
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	// +kubebuilder:validation:Optional
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	// +kubebuilder:validation:Optional
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	// +kubebuilder:validation:Optional
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	// +kubebuilder:validation:Optional
	LoggingConfig []LoggingConfigParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
	// +kubebuilder:validation:Optional
	MainClass *string `json:"mainClass,omitempty" tf:"main_class,omitempty"`

	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
	// +kubebuilder:validation:Optional
	MainJarFileURI *string `json:"mainJarFileUri,omitempty" tf:"main_jar_file_uri,omitempty"`

	// A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`
}

type HiveConfigInitParameters struct {

	// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	// A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS URI of file containing Hive script to execute as the job.
	// Conflicts with query_list
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of Hive queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type HiveConfigObservation struct {

	// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	// A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS URI of file containing Hive script to execute as the job.
	// Conflicts with query_list
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of Hive queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type HiveConfigParameters struct {

	// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	// +kubebuilder:validation:Optional
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	// +kubebuilder:validation:Optional
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	// A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS URI of file containing Hive script to execute as the job.
	// Conflicts with query_list
	// +kubebuilder:validation:Optional
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of Hive queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	// +kubebuilder:validation:Optional
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	// +kubebuilder:validation:Optional
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type JobInitParameters struct {

	// By default, you can only delete inactive jobs within
	// Dataproc. Setting this to true, and calling destroy, will ensure that the
	// job is first cancelled before issuing the delete.
	ForceDelete *bool `json:"forceDelete,omitempty" tf:"force_delete,omitempty"`

	HadoopConfig []HadoopConfigInitParameters `json:"hadoopConfig,omitempty" tf:"hadoop_config,omitempty"`

	HiveConfig []HiveConfigInitParameters `json:"hiveConfig,omitempty" tf:"hive_config,omitempty"`

	// The list of labels (key/value pairs) to add to the job.
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	PigConfig []PigConfigInitParameters `json:"pigConfig,omitempty" tf:"pig_config,omitempty"`

	Placement []PlacementInitParameters `json:"placement,omitempty" tf:"placement,omitempty"`

	PrestoConfig []PrestoConfigInitParameters `json:"prestoConfig,omitempty" tf:"presto_config,omitempty"`

	// The project in which the cluster can be found and jobs
	// subsequently run against. If it is not provided, the provider project is used.
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	PysparkConfig []PysparkConfigInitParameters `json:"pysparkConfig,omitempty" tf:"pyspark_config,omitempty"`

	Reference []ReferenceInitParameters `json:"reference,omitempty" tf:"reference,omitempty"`

	// The Cloud Dataproc region. This essentially determines which clusters are available
	// for this job to be submitted to. If not specified, defaults to global.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/dataproc/v1beta1.Cluster
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractParamPath("region",false)
	Region *string `json:"region,omitempty" tf:"region,omitempty"`

	// Reference to a Cluster in dataproc to populate region.
	// +kubebuilder:validation:Optional
	RegionRef *v1.Reference `json:"regionRef,omitempty" tf:"-"`

	// Selector for a Cluster in dataproc to populate region.
	// +kubebuilder:validation:Optional
	RegionSelector *v1.Selector `json:"regionSelector,omitempty" tf:"-"`

	Scheduling []SchedulingInitParameters `json:"scheduling,omitempty" tf:"scheduling,omitempty"`

	SparkConfig []SparkConfigInitParameters `json:"sparkConfig,omitempty" tf:"spark_config,omitempty"`

	SparksqlConfig []SparksqlConfigInitParameters `json:"sparksqlConfig,omitempty" tf:"sparksql_config,omitempty"`
}

type JobObservation struct {

	// If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
	DriverControlsFilesURI *string `json:"driverControlsFilesUri,omitempty" tf:"driver_controls_files_uri,omitempty"`

	// A URI pointing to the location of the stdout of the job's driver program.
	DriverOutputResourceURI *string `json:"driverOutputResourceUri,omitempty" tf:"driver_output_resource_uri,omitempty"`

	// By default, you can only delete inactive jobs within
	// Dataproc. Setting this to true, and calling destroy, will ensure that the
	// job is first cancelled before issuing the delete.
	ForceDelete *bool `json:"forceDelete,omitempty" tf:"force_delete,omitempty"`

	HadoopConfig []HadoopConfigObservation `json:"hadoopConfig,omitempty" tf:"hadoop_config,omitempty"`

	HiveConfig []HiveConfigObservation `json:"hiveConfig,omitempty" tf:"hive_config,omitempty"`

	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// The list of labels (key/value pairs) to add to the job.
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	PigConfig []PigConfigObservation `json:"pigConfig,omitempty" tf:"pig_config,omitempty"`

	Placement []PlacementObservation `json:"placement,omitempty" tf:"placement,omitempty"`

	PrestoConfig []PrestoConfigObservation `json:"prestoConfig,omitempty" tf:"presto_config,omitempty"`

	// The project in which the cluster can be found and jobs
	// subsequently run against. If it is not provided, the provider project is used.
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	PysparkConfig []PysparkConfigObservation `json:"pysparkConfig,omitempty" tf:"pyspark_config,omitempty"`

	Reference []ReferenceObservation `json:"reference,omitempty" tf:"reference,omitempty"`

	// The Cloud Dataproc region. This essentially determines which clusters are available
	// for this job to be submitted to. If not specified, defaults to global.
	Region *string `json:"region,omitempty" tf:"region,omitempty"`

	Scheduling []SchedulingObservation `json:"scheduling,omitempty" tf:"scheduling,omitempty"`

	SparkConfig []SparkConfigObservation `json:"sparkConfig,omitempty" tf:"spark_config,omitempty"`

	SparksqlConfig []SparksqlConfigObservation `json:"sparksqlConfig,omitempty" tf:"sparksql_config,omitempty"`

	Status []StatusObservation `json:"status,omitempty" tf:"status,omitempty"`
}

type JobParameters struct {

	// By default, you can only delete inactive jobs within
	// Dataproc. Setting this to true, and calling destroy, will ensure that the
	// job is first cancelled before issuing the delete.
	// +kubebuilder:validation:Optional
	ForceDelete *bool `json:"forceDelete,omitempty" tf:"force_delete,omitempty"`

	// +kubebuilder:validation:Optional
	HadoopConfig []HadoopConfigParameters `json:"hadoopConfig,omitempty" tf:"hadoop_config,omitempty"`

	// +kubebuilder:validation:Optional
	HiveConfig []HiveConfigParameters `json:"hiveConfig,omitempty" tf:"hive_config,omitempty"`

	// The list of labels (key/value pairs) to add to the job.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	// +kubebuilder:validation:Optional
	PigConfig []PigConfigParameters `json:"pigConfig,omitempty" tf:"pig_config,omitempty"`

	// +kubebuilder:validation:Optional
	Placement []PlacementParameters `json:"placement,omitempty" tf:"placement,omitempty"`

	// +kubebuilder:validation:Optional
	PrestoConfig []PrestoConfigParameters `json:"prestoConfig,omitempty" tf:"presto_config,omitempty"`

	// The project in which the cluster can be found and jobs
	// subsequently run against. If it is not provided, the provider project is used.
	// +kubebuilder:validation:Optional
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	// +kubebuilder:validation:Optional
	PysparkConfig []PysparkConfigParameters `json:"pysparkConfig,omitempty" tf:"pyspark_config,omitempty"`

	// +kubebuilder:validation:Optional
	Reference []ReferenceParameters `json:"reference,omitempty" tf:"reference,omitempty"`

	// The Cloud Dataproc region. This essentially determines which clusters are available
	// for this job to be submitted to. If not specified, defaults to global.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/dataproc/v1beta1.Cluster
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractParamPath("region",false)
	// +kubebuilder:validation:Optional
	Region *string `json:"region,omitempty" tf:"region,omitempty"`

	// Reference to a Cluster in dataproc to populate region.
	// +kubebuilder:validation:Optional
	RegionRef *v1.Reference `json:"regionRef,omitempty" tf:"-"`

	// Selector for a Cluster in dataproc to populate region.
	// +kubebuilder:validation:Optional
	RegionSelector *v1.Selector `json:"regionSelector,omitempty" tf:"-"`

	// +kubebuilder:validation:Optional
	Scheduling []SchedulingParameters `json:"scheduling,omitempty" tf:"scheduling,omitempty"`

	// +kubebuilder:validation:Optional
	SparkConfig []SparkConfigParameters `json:"sparkConfig,omitempty" tf:"spark_config,omitempty"`

	// +kubebuilder:validation:Optional
	SparksqlConfig []SparksqlConfigParameters `json:"sparksqlConfig,omitempty" tf:"sparksql_config,omitempty"`
}

type LoggingConfigInitParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type LoggingConfigObservation struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type LoggingConfigParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +kubebuilder:validation:Optional
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels" tf:"driver_log_levels,omitempty"`
}

type PigConfigInitParameters struct {

	// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []PigConfigLoggingConfigInitParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS URI of file containing Hive script to execute as the job.
	// Conflicts with query_list
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of Hive queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type PigConfigLoggingConfigInitParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type PigConfigLoggingConfigObservation struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type PigConfigLoggingConfigParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +kubebuilder:validation:Optional
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels" tf:"driver_log_levels,omitempty"`
}

type PigConfigObservation struct {

	// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []PigConfigLoggingConfigObservation `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS URI of file containing Hive script to execute as the job.
	// Conflicts with query_list
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of Hive queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type PigConfigParameters struct {

	// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	// +kubebuilder:validation:Optional
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	// +kubebuilder:validation:Optional
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	// +kubebuilder:validation:Optional
	LoggingConfig []PigConfigLoggingConfigParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS URI of file containing Hive script to execute as the job.
	// Conflicts with query_list
	// +kubebuilder:validation:Optional
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of Hive queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	// +kubebuilder:validation:Optional
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	// +kubebuilder:validation:Optional
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type PlacementInitParameters struct {

	// The name of the cluster where the job
	// will be submitted.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/dataproc/v1beta1.Cluster
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractParamPath("name",false)
	ClusterName *string `json:"clusterName,omitempty" tf:"cluster_name,omitempty"`

	// Reference to a Cluster in dataproc to populate clusterName.
	// +kubebuilder:validation:Optional
	ClusterNameRef *v1.Reference `json:"clusterNameRef,omitempty" tf:"-"`

	// Selector for a Cluster in dataproc to populate clusterName.
	// +kubebuilder:validation:Optional
	ClusterNameSelector *v1.Selector `json:"clusterNameSelector,omitempty" tf:"-"`
}

type PlacementObservation struct {

	// The name of the cluster where the job
	// will be submitted.
	ClusterName *string `json:"clusterName,omitempty" tf:"cluster_name,omitempty"`

	// A cluster UUID generated by the Cloud Dataproc service when the job is submitted.
	ClusterUUID *string `json:"clusterUuid,omitempty" tf:"cluster_uuid,omitempty"`
}

type PlacementParameters struct {

	// The name of the cluster where the job
	// will be submitted.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/dataproc/v1beta1.Cluster
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractParamPath("name",false)
	// +kubebuilder:validation:Optional
	ClusterName *string `json:"clusterName,omitempty" tf:"cluster_name,omitempty"`

	// Reference to a Cluster in dataproc to populate clusterName.
	// +kubebuilder:validation:Optional
	ClusterNameRef *v1.Reference `json:"clusterNameRef,omitempty" tf:"-"`

	// Selector for a Cluster in dataproc to populate clusterName.
	// +kubebuilder:validation:Optional
	ClusterNameSelector *v1.Selector `json:"clusterNameSelector,omitempty" tf:"-"`
}

type PrestoConfigInitParameters struct {

	// Presto client tags to attach to this query.
	ClientTags []*string `json:"clientTags,omitempty" tf:"client_tags,omitempty"`

	// Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	LoggingConfig []PrestoConfigLoggingConfigInitParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The format in which query output will be displayed. See the Presto documentation for supported output formats.
	OutputFormat *string `json:"outputFormat,omitempty" tf:"output_format,omitempty"`

	// A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// The HCFS URI of the script that contains SQL queries.
	// Conflicts with query_list
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of SQL queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`
}

type PrestoConfigLoggingConfigInitParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type PrestoConfigLoggingConfigObservation struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type PrestoConfigLoggingConfigParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +kubebuilder:validation:Optional
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels" tf:"driver_log_levels,omitempty"`
}

type PrestoConfigObservation struct {

	// Presto client tags to attach to this query.
	ClientTags []*string `json:"clientTags,omitempty" tf:"client_tags,omitempty"`

	// Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	LoggingConfig []PrestoConfigLoggingConfigObservation `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The format in which query output will be displayed. See the Presto documentation for supported output formats.
	OutputFormat *string `json:"outputFormat,omitempty" tf:"output_format,omitempty"`

	// A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// The HCFS URI of the script that contains SQL queries.
	// Conflicts with query_list
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of SQL queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`
}

type PrestoConfigParameters struct {

	// Presto client tags to attach to this query.
	// +kubebuilder:validation:Optional
	ClientTags []*string `json:"clientTags,omitempty" tf:"client_tags,omitempty"`

	// Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
	// +kubebuilder:validation:Optional
	ContinueOnFailure *bool `json:"continueOnFailure,omitempty" tf:"continue_on_failure,omitempty"`

	// +kubebuilder:validation:Optional
	LoggingConfig []PrestoConfigLoggingConfigParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The format in which query output will be displayed. See the Presto documentation for supported output formats.
	// +kubebuilder:validation:Optional
	OutputFormat *string `json:"outputFormat,omitempty" tf:"output_format,omitempty"`

	// A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// The HCFS URI of the script that contains SQL queries.
	// Conflicts with query_list
	// +kubebuilder:validation:Optional
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of SQL queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	// +kubebuilder:validation:Optional
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`
}

type PysparkConfigInitParameters struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver.
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []PysparkConfigLoggingConfigInitParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileURI *string `json:"mainPythonFileUri,omitempty" tf:"main_python_file_uri,omitempty"`

	// A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []*string `json:"pythonFileUris,omitempty" tf:"python_file_uris,omitempty"`
}

type PysparkConfigLoggingConfigInitParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type PysparkConfigLoggingConfigObservation struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type PysparkConfigLoggingConfigParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +kubebuilder:validation:Optional
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels" tf:"driver_log_levels,omitempty"`
}

type PysparkConfigObservation struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver.
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []PysparkConfigLoggingConfigObservation `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileURI *string `json:"mainPythonFileUri,omitempty" tf:"main_python_file_uri,omitempty"`

	// A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []*string `json:"pythonFileUris,omitempty" tf:"python_file_uris,omitempty"`
}

type PysparkConfigParameters struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	// +kubebuilder:validation:Optional
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver.
	// +kubebuilder:validation:Optional
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
	// +kubebuilder:validation:Optional
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	// +kubebuilder:validation:Optional
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	// +kubebuilder:validation:Optional
	LoggingConfig []PysparkConfigLoggingConfigParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	// +kubebuilder:validation:Optional
	MainPythonFileURI *string `json:"mainPythonFileUri" tf:"main_python_file_uri,omitempty"`

	// A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	// +kubebuilder:validation:Optional
	PythonFileUris []*string `json:"pythonFileUris,omitempty" tf:"python_file_uris,omitempty"`
}

type ReferenceInitParameters struct {
	JobID *string `json:"jobId,omitempty" tf:"job_id,omitempty"`
}

type ReferenceObservation struct {
	JobID *string `json:"jobId,omitempty" tf:"job_id,omitempty"`
}

type ReferenceParameters struct {

	// +kubebuilder:validation:Optional
	JobID *string `json:"jobId,omitempty" tf:"job_id,omitempty"`
}

type SchedulingInitParameters struct {

	// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
	MaxFailuresPerHour *float64 `json:"maxFailuresPerHour,omitempty" tf:"max_failures_per_hour,omitempty"`

	// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
	MaxFailuresTotal *float64 `json:"maxFailuresTotal,omitempty" tf:"max_failures_total,omitempty"`
}

type SchedulingObservation struct {

	// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
	MaxFailuresPerHour *float64 `json:"maxFailuresPerHour,omitempty" tf:"max_failures_per_hour,omitempty"`

	// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
	MaxFailuresTotal *float64 `json:"maxFailuresTotal,omitempty" tf:"max_failures_total,omitempty"`
}

type SchedulingParameters struct {

	// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
	// +kubebuilder:validation:Optional
	MaxFailuresPerHour *float64 `json:"maxFailuresPerHour" tf:"max_failures_per_hour,omitempty"`

	// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
	// +kubebuilder:validation:Optional
	MaxFailuresTotal *float64 `json:"maxFailuresTotal" tf:"max_failures_total,omitempty"`
}

type SparkConfigInitParameters struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver.
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []SparkConfigLoggingConfigInitParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The class containing the main method of the driver. Must be in a
	// provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
	MainClass *string `json:"mainClass,omitempty" tf:"main_class,omitempty"`

	// The HCFS URI of jar file containing
	// the driver jar. Conflicts with main_class
	MainJarFileURI *string `json:"mainJarFileUri,omitempty" tf:"main_jar_file_uri,omitempty"`

	// A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`
}

type SparkConfigLoggingConfigInitParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type SparkConfigLoggingConfigObservation struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type SparkConfigLoggingConfigParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +kubebuilder:validation:Optional
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels" tf:"driver_log_levels,omitempty"`
}

type SparkConfigObservation struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver.
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []SparkConfigLoggingConfigObservation `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The class containing the main method of the driver. Must be in a
	// provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
	MainClass *string `json:"mainClass,omitempty" tf:"main_class,omitempty"`

	// The HCFS URI of jar file containing
	// the driver jar. Conflicts with main_class
	MainJarFileURI *string `json:"mainJarFileUri,omitempty" tf:"main_jar_file_uri,omitempty"`

	// A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`
}

type SparkConfigParameters struct {

	// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
	// +kubebuilder:validation:Optional
	ArchiveUris []*string `json:"archiveUris,omitempty" tf:"archive_uris,omitempty"`

	// The arguments to pass to the driver.
	// +kubebuilder:validation:Optional
	Args []*string `json:"args,omitempty" tf:"args,omitempty"`

	// HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
	// +kubebuilder:validation:Optional
	FileUris []*string `json:"fileUris,omitempty" tf:"file_uris,omitempty"`

	// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	// +kubebuilder:validation:Optional
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	// +kubebuilder:validation:Optional
	LoggingConfig []SparkConfigLoggingConfigParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// The class containing the main method of the driver. Must be in a
	// provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
	// +kubebuilder:validation:Optional
	MainClass *string `json:"mainClass,omitempty" tf:"main_class,omitempty"`

	// The HCFS URI of jar file containing
	// the driver jar. Conflicts with main_class
	// +kubebuilder:validation:Optional
	MainJarFileURI *string `json:"mainJarFileUri,omitempty" tf:"main_jar_file_uri,omitempty"`

	// A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`
}

type SparksqlConfigInitParameters struct {

	// HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []SparksqlConfigLoggingConfigInitParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// The HCFS URI of the script that contains SQL queries.
	// Conflicts with query_list
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of SQL queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type SparksqlConfigLoggingConfigInitParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type SparksqlConfigLoggingConfigObservation struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels,omitempty" tf:"driver_log_levels,omitempty"`
}

type SparksqlConfigLoggingConfigParameters struct {

	// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	// +kubebuilder:validation:Optional
	// +mapType=granular
	DriverLogLevels map[string]*string `json:"driverLogLevels" tf:"driver_log_levels,omitempty"`
}

type SparksqlConfigObservation struct {

	// HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	LoggingConfig []SparksqlConfigLoggingConfigObservation `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// The HCFS URI of the script that contains SQL queries.
	// Conflicts with query_list
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of SQL queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type SparksqlConfigParameters struct {

	// HCFS URIs of jar files to be added to the Spark CLASSPATH.
	// +kubebuilder:validation:Optional
	JarFileUris []*string `json:"jarFileUris,omitempty" tf:"jar_file_uris,omitempty"`

	// +kubebuilder:validation:Optional
	LoggingConfig []SparksqlConfigLoggingConfigParameters `json:"loggingConfig,omitempty" tf:"logging_config,omitempty"`

	// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Properties map[string]*string `json:"properties,omitempty" tf:"properties,omitempty"`

	// The HCFS URI of the script that contains SQL queries.
	// Conflicts with query_list
	// +kubebuilder:validation:Optional
	QueryFileURI *string `json:"queryFileUri,omitempty" tf:"query_file_uri,omitempty"`

	// The list of SQL queries or statements to execute as part of the job.
	// Conflicts with query_file_uri
	// +kubebuilder:validation:Optional
	QueryList []*string `json:"queryList,omitempty" tf:"query_list,omitempty"`

	// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	// +kubebuilder:validation:Optional
	// +mapType=granular
	ScriptVariables map[string]*string `json:"scriptVariables,omitempty" tf:"script_variables,omitempty"`
}

type StatusInitParameters struct {
}

type StatusObservation struct {

	// Optional job state details, such as an error description if the state is ERROR.
	Details *string `json:"details,omitempty" tf:"details,omitempty"`

	// A state message specifying the overall job state.
	State *string `json:"state,omitempty" tf:"state,omitempty"`

	// The time when this state was entered.
	StateStartTime *string `json:"stateStartTime,omitempty" tf:"state_start_time,omitempty"`

	// Additional state information, which includes status reported by the agent.
	Substate *string `json:"substate,omitempty" tf:"substate,omitempty"`
}

type StatusParameters struct {
}

// JobSpec defines the desired state of Job
type JobSpec struct {
	v1.ResourceSpec `json:",inline"`
	ForProvider     JobParameters `json:"forProvider"`
	// THIS IS A BETA FIELD. It will be honored
	// unless the Management Policies feature flag is disabled.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	InitProvider JobInitParameters `json:"initProvider,omitempty"`
}

// JobStatus defines the observed state of Job.
type JobStatus struct {
	v1.ResourceStatus `json:",inline"`
	AtProvider        JobObservation `json:"atProvider,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:storageversion

// Job is the Schema for the Jobs API. Manages a job resource within a Dataproc cluster.
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,gcp}
type Job struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.placement) || (has(self.initProvider) && has(self.initProvider.placement))",message="spec.forProvider.placement is a required parameter"
	Spec   JobSpec   `json:"spec"`
	Status JobStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// JobList contains a list of Jobs
type JobList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Job `json:"items"`
}

// Repository type metadata.
var (
	Job_Kind             = "Job"
	Job_GroupKind        = schema.GroupKind{Group: CRDGroup, Kind: Job_Kind}.String()
	Job_KindAPIVersion   = Job_Kind + "." + CRDGroupVersion.String()
	Job_GroupVersionKind = CRDGroupVersion.WithKind(Job_Kind)
)

func init() {
	SchemeBuilder.Register(&Job{}, &JobList{})
}

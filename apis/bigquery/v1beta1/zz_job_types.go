// SPDX-FileCopyrightText: 2024 The Crossplane Authors <https://crossplane.io>
//
// SPDX-License-Identifier: Apache-2.0

// Code generated by upjet. DO NOT EDIT.

package v1beta1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	v1 "github.com/crossplane/crossplane-runtime/apis/common/v1"
)

type CopyInitParameters struct {

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	DestinationEncryptionConfiguration []DestinationEncryptionConfigurationInitParameters `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// The destination table.
	// Structure is documented below.
	DestinationTable []DestinationTableInitParameters `json:"destinationTable,omitempty" tf:"destination_table,omitempty"`

	// Source tables to copy.
	// Structure is documented below.
	SourceTables []SourceTablesInitParameters `json:"sourceTables,omitempty" tf:"source_tables,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type CopyObservation struct {

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	DestinationEncryptionConfiguration []DestinationEncryptionConfigurationObservation `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// The destination table.
	// Structure is documented below.
	DestinationTable []DestinationTableObservation `json:"destinationTable,omitempty" tf:"destination_table,omitempty"`

	// Source tables to copy.
	// Structure is documented below.
	SourceTables []SourceTablesObservation `json:"sourceTables,omitempty" tf:"source_tables,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type CopyParameters struct {

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	// +kubebuilder:validation:Optional
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	DestinationEncryptionConfiguration []DestinationEncryptionConfigurationParameters `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// The destination table.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	DestinationTable []DestinationTableParameters `json:"destinationTable,omitempty" tf:"destination_table,omitempty"`

	// Source tables to copy.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	SourceTables []SourceTablesParameters `json:"sourceTables" tf:"source_tables,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	// +kubebuilder:validation:Optional
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type DefaultDatasetInitParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`
}

type DefaultDatasetObservation struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`
}

type DefaultDatasetParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`
}

type DestinationEncryptionConfigurationInitParameters struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/kms/v1beta1.CryptoKey
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`

	// Reference to a CryptoKey in kms to populate kmsKeyName.
	// +kubebuilder:validation:Optional
	KMSKeyNameRef *v1.Reference `json:"kmsKeyNameRef,omitempty" tf:"-"`

	// Selector for a CryptoKey in kms to populate kmsKeyName.
	// +kubebuilder:validation:Optional
	KMSKeyNameSelector *v1.Selector `json:"kmsKeyNameSelector,omitempty" tf:"-"`
}

type DestinationEncryptionConfigurationObservation struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`

	// (Output)
	// Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
	KMSKeyVersion *string `json:"kmsKeyVersion,omitempty" tf:"kms_key_version,omitempty"`
}

type DestinationEncryptionConfigurationParameters struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/kms/v1beta1.CryptoKey
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`

	// Reference to a CryptoKey in kms to populate kmsKeyName.
	// +kubebuilder:validation:Optional
	KMSKeyNameRef *v1.Reference `json:"kmsKeyNameRef,omitempty" tf:"-"`

	// Selector for a CryptoKey in kms to populate kmsKeyName.
	// +kubebuilder:validation:Optional
	KMSKeyNameSelector *v1.Selector `json:"kmsKeyNameSelector,omitempty" tf:"-"`
}

type DestinationTableInitParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`

	// Reference to a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDRef *v1.Reference `json:"tableIdRef,omitempty" tf:"-"`

	// Selector for a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDSelector *v1.Selector `json:"tableIdSelector,omitempty" tf:"-"`
}

type DestinationTableObservation struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`
}

type DestinationTableParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +kubebuilder:validation:Optional
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`

	// Reference to a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDRef *v1.Reference `json:"tableIdRef,omitempty" tf:"-"`

	// Selector for a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDSelector *v1.Selector `json:"tableIdSelector,omitempty" tf:"-"`
}

type ErrorResultInitParameters struct {
}

type ErrorResultObservation struct {

	// The geographic location of the job. The default value is US.
	Location *string `json:"location,omitempty" tf:"location,omitempty"`

	// A human-readable description of the error.
	Message *string `json:"message,omitempty" tf:"message,omitempty"`

	// A short error code that summarizes the error.
	Reason *string `json:"reason,omitempty" tf:"reason,omitempty"`
}

type ErrorResultParameters struct {
}

type ErrorsInitParameters struct {
}

type ErrorsObservation struct {

	// The geographic location of the job. The default value is US.
	Location *string `json:"location,omitempty" tf:"location,omitempty"`

	// A human-readable description of the error.
	Message *string `json:"message,omitempty" tf:"message,omitempty"`

	// A short error code that summarizes the error.
	Reason *string `json:"reason,omitempty" tf:"reason,omitempty"`
}

type ErrorsParameters struct {
}

type ExtractInitParameters struct {

	// The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
	// The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
	Compression *string `json:"compression,omitempty" tf:"compression,omitempty"`

	// The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
	// The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
	// The default value for models is SAVED_MODEL.
	DestinationFormat *string `json:"destinationFormat,omitempty" tf:"destination_format,omitempty"`

	// A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
	DestinationUris []*string `json:"destinationUris,omitempty" tf:"destination_uris,omitempty"`

	// When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
	// Default is ','
	FieldDelimiter *string `json:"fieldDelimiter,omitempty" tf:"field_delimiter,omitempty"`

	// Whether to print out a header row in the results. Default is true.
	PrintHeader *bool `json:"printHeader,omitempty" tf:"print_header,omitempty"`

	// A reference to the model being exported.
	// Structure is documented below.
	SourceModel []SourceModelInitParameters `json:"sourceModel,omitempty" tf:"source_model,omitempty"`

	// A reference to the table being exported.
	// Structure is documented below.
	SourceTable []SourceTableInitParameters `json:"sourceTable,omitempty" tf:"source_table,omitempty"`

	// Whether to use logical types when extracting to AVRO format.
	UseAvroLogicalTypes *bool `json:"useAvroLogicalTypes,omitempty" tf:"use_avro_logical_types,omitempty"`
}

type ExtractObservation struct {

	// The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
	// The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
	Compression *string `json:"compression,omitempty" tf:"compression,omitempty"`

	// The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
	// The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
	// The default value for models is SAVED_MODEL.
	DestinationFormat *string `json:"destinationFormat,omitempty" tf:"destination_format,omitempty"`

	// A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
	DestinationUris []*string `json:"destinationUris,omitempty" tf:"destination_uris,omitempty"`

	// When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
	// Default is ','
	FieldDelimiter *string `json:"fieldDelimiter,omitempty" tf:"field_delimiter,omitempty"`

	// Whether to print out a header row in the results. Default is true.
	PrintHeader *bool `json:"printHeader,omitempty" tf:"print_header,omitempty"`

	// A reference to the model being exported.
	// Structure is documented below.
	SourceModel []SourceModelObservation `json:"sourceModel,omitempty" tf:"source_model,omitempty"`

	// A reference to the table being exported.
	// Structure is documented below.
	SourceTable []SourceTableObservation `json:"sourceTable,omitempty" tf:"source_table,omitempty"`

	// Whether to use logical types when extracting to AVRO format.
	UseAvroLogicalTypes *bool `json:"useAvroLogicalTypes,omitempty" tf:"use_avro_logical_types,omitempty"`
}

type ExtractParameters struct {

	// The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
	// The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
	// +kubebuilder:validation:Optional
	Compression *string `json:"compression,omitempty" tf:"compression,omitempty"`

	// The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
	// The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
	// The default value for models is SAVED_MODEL.
	// +kubebuilder:validation:Optional
	DestinationFormat *string `json:"destinationFormat,omitempty" tf:"destination_format,omitempty"`

	// A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
	// +kubebuilder:validation:Optional
	DestinationUris []*string `json:"destinationUris" tf:"destination_uris,omitempty"`

	// When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
	// Default is ','
	// +kubebuilder:validation:Optional
	FieldDelimiter *string `json:"fieldDelimiter,omitempty" tf:"field_delimiter,omitempty"`

	// Whether to print out a header row in the results. Default is true.
	// +kubebuilder:validation:Optional
	PrintHeader *bool `json:"printHeader,omitempty" tf:"print_header,omitempty"`

	// A reference to the model being exported.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	SourceModel []SourceModelParameters `json:"sourceModel,omitempty" tf:"source_model,omitempty"`

	// A reference to the table being exported.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	SourceTable []SourceTableParameters `json:"sourceTable,omitempty" tf:"source_table,omitempty"`

	// Whether to use logical types when extracting to AVRO format.
	// +kubebuilder:validation:Optional
	UseAvroLogicalTypes *bool `json:"useAvroLogicalTypes,omitempty" tf:"use_avro_logical_types,omitempty"`
}

type JobInitParameters struct {

	// Copies a table.
	// Structure is documented below.
	Copy []CopyInitParameters `json:"copy,omitempty" tf:"copy,omitempty"`

	// Configures an extract job.
	// Structure is documented below.
	Extract []ExtractInitParameters `json:"extract,omitempty" tf:"extract,omitempty"`

	// The ID of the job. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024 characters.
	JobID *string `json:"jobId,omitempty" tf:"job_id,omitempty"`

	// Job timeout in milliseconds. If this time limit is exceeded, BigQuery may attempt to terminate the job.
	JobTimeoutMs *string `json:"jobTimeoutMs,omitempty" tf:"job_timeout_ms,omitempty"`

	// The labels associated with this job. You can use these to organize and group your jobs.
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	// Configures a load job.
	// Structure is documented below.
	Load []LoadInitParameters `json:"load,omitempty" tf:"load,omitempty"`

	// The geographic location of the job. The default value is US.
	Location *string `json:"location,omitempty" tf:"location,omitempty"`

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	// Configures a query job.
	// Structure is documented below.
	Query []QueryInitParameters `json:"query,omitempty" tf:"query,omitempty"`
}

type JobObservation struct {

	// Copies a table.
	// Structure is documented below.
	Copy []CopyObservation `json:"copy,omitempty" tf:"copy,omitempty"`

	// +mapType=granular
	EffectiveLabels map[string]*string `json:"effectiveLabels,omitempty" tf:"effective_labels,omitempty"`

	// Configures an extract job.
	// Structure is documented below.
	Extract []ExtractObservation `json:"extract,omitempty" tf:"extract,omitempty"`

	// an identifier for the resource with format projects/{{project}}/jobs/{{job_id}}
	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// The ID of the job. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024 characters.
	JobID *string `json:"jobId,omitempty" tf:"job_id,omitempty"`

	// Job timeout in milliseconds. If this time limit is exceeded, BigQuery may attempt to terminate the job.
	JobTimeoutMs *string `json:"jobTimeoutMs,omitempty" tf:"job_timeout_ms,omitempty"`

	// (Output)
	// The type of the job.
	JobType *string `json:"jobType,omitempty" tf:"job_type,omitempty"`

	// The labels associated with this job. You can use these to organize and group your jobs.
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	// Configures a load job.
	// Structure is documented below.
	Load []LoadObservation `json:"load,omitempty" tf:"load,omitempty"`

	// The geographic location of the job. The default value is US.
	Location *string `json:"location,omitempty" tf:"location,omitempty"`

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	// Configures a query job.
	// Structure is documented below.
	Query []QueryObservation `json:"query,omitempty" tf:"query,omitempty"`

	// The status of this job. Examine this value when polling an asynchronous job to see if the job is complete.
	// Structure is documented below.
	Status []StatusObservation `json:"status,omitempty" tf:"status,omitempty"`

	// (Output)
	// The combination of labels configured directly on the resource
	// and default labels configured on the provider.
	// +mapType=granular
	TerraformLabels map[string]*string `json:"terraformLabels,omitempty" tf:"terraform_labels,omitempty"`

	// Email address of the user who ran the job.
	UserEmail *string `json:"userEmail,omitempty" tf:"user_email,omitempty"`
}

type JobParameters struct {

	// Copies a table.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	Copy []CopyParameters `json:"copy,omitempty" tf:"copy,omitempty"`

	// Configures an extract job.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	Extract []ExtractParameters `json:"extract,omitempty" tf:"extract,omitempty"`

	// The ID of the job. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024 characters.
	// +kubebuilder:validation:Optional
	JobID *string `json:"jobId,omitempty" tf:"job_id,omitempty"`

	// Job timeout in milliseconds. If this time limit is exceeded, BigQuery may attempt to terminate the job.
	// +kubebuilder:validation:Optional
	JobTimeoutMs *string `json:"jobTimeoutMs,omitempty" tf:"job_timeout_ms,omitempty"`

	// The labels associated with this job. You can use these to organize and group your jobs.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	// Configures a load job.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	Load []LoadParameters `json:"load,omitempty" tf:"load,omitempty"`

	// The geographic location of the job. The default value is US.
	// +kubebuilder:validation:Optional
	Location *string `json:"location,omitempty" tf:"location,omitempty"`

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	// +kubebuilder:validation:Optional
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	// Configures a query job.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	Query []QueryParameters `json:"query,omitempty" tf:"query,omitempty"`
}

type LoadDestinationEncryptionConfigurationInitParameters struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`
}

type LoadDestinationEncryptionConfigurationObservation struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`

	// (Output)
	// Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
	KMSKeyVersion *string `json:"kmsKeyVersion,omitempty" tf:"kms_key_version,omitempty"`
}

type LoadDestinationEncryptionConfigurationParameters struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	// +kubebuilder:validation:Optional
	KMSKeyName *string `json:"kmsKeyName" tf:"kms_key_name,omitempty"`
}

type LoadDestinationTableInitParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`

	// Reference to a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDRef *v1.Reference `json:"tableIdRef,omitempty" tf:"-"`

	// Selector for a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDSelector *v1.Selector `json:"tableIdSelector,omitempty" tf:"-"`
}

type LoadDestinationTableObservation struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`
}

type LoadDestinationTableParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +kubebuilder:validation:Optional
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`

	// Reference to a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDRef *v1.Reference `json:"tableIdRef,omitempty" tf:"-"`

	// Selector for a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDSelector *v1.Selector `json:"tableIdSelector,omitempty" tf:"-"`
}

type LoadInitParameters struct {

	// Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
	// If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
	// an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
	AllowJaggedRows *bool `json:"allowJaggedRows,omitempty" tf:"allow_jagged_rows,omitempty"`

	// Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
	// The default value is false.
	AllowQuotedNewlines *bool `json:"allowQuotedNewlines,omitempty" tf:"allow_quoted_newlines,omitempty"`

	// Indicates if we should automatically infer the options and schema for CSV and JSON sources.
	Autodetect *bool `json:"autodetect,omitempty" tf:"autodetect,omitempty"`

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	DestinationEncryptionConfiguration []LoadDestinationEncryptionConfigurationInitParameters `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// The destination table to load the data into.
	// Structure is documented below.
	DestinationTable []LoadDestinationTableInitParameters `json:"destinationTable,omitempty" tf:"destination_table,omitempty"`

	// The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
	// The default value is UTF-8. BigQuery decodes the data after the raw, binary data
	// has been split using the values of the quote and fieldDelimiter properties.
	Encoding *string `json:"encoding,omitempty" tf:"encoding,omitempty"`

	// The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
	// To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
	// the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
	// data in its raw, binary state. BigQuery also supports the escape sequence "\t" to specify a tab separator.
	// The default value is a comma (',').
	FieldDelimiter *string `json:"fieldDelimiter,omitempty" tf:"field_delimiter,omitempty"`

	// Indicates if BigQuery should allow extra values that are not represented in the table schema.
	// If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
	// and if there are too many bad records, an invalid error is returned in the job result.
	// The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
	// CSV: Trailing columns
	// JSON: Named values that don't match any column names
	IgnoreUnknownValues *bool `json:"ignoreUnknownValues,omitempty" tf:"ignore_unknown_values,omitempty"`

	// If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
	// For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
	// GeoJSON: set to GEOJSON.
	JSONExtension *string `json:"jsonExtension,omitempty" tf:"json_extension,omitempty"`

	// The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
	// an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
	MaxBadRecords *float64 `json:"maxBadRecords,omitempty" tf:"max_bad_records,omitempty"`

	// Specifies a string that represents a null value in a CSV file. For example, if you specify "\N", BigQuery interprets "\N" as a null value
	// when loading a CSV file. The default value is the empty string. If you set this property to a custom value, BigQuery throws an error if an
	// empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
	// an empty value.
	NullMarker *string `json:"nullMarker,omitempty" tf:"null_marker,omitempty"`

	// Parquet Options for load and make external tables.
	// Structure is documented below.
	ParquetOptions []ParquetOptionsInitParameters `json:"parquetOptions,omitempty" tf:"parquet_options,omitempty"`

	// If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
	// Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
	// If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
	ProjectionFields []*string `json:"projectionFields,omitempty" tf:"projection_fields,omitempty"`

	// The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
	// and then uses the first byte of the encoded string to split the data in its raw, binary state.
	// The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
	// If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
	Quote *string `json:"quote,omitempty" tf:"quote,omitempty"`

	// Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
	// supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
	// For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	SchemaUpdateOptions []*string `json:"schemaUpdateOptions,omitempty" tf:"schema_update_options,omitempty"`

	// The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
	// The default value is 0. This property is useful if you have header rows in the file that should be skipped.
	// When autodetect is on, the behavior is the following:
	// skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
	// the row is read as data. Otherwise data is read starting from the second row.
	// skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
	// skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
	// row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
	SkipLeadingRows *float64 `json:"skipLeadingRows,omitempty" tf:"skip_leading_rows,omitempty"`

	// The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
	// For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
	// For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
	// The default value is CSV.
	SourceFormat *string `json:"sourceFormat,omitempty" tf:"source_format,omitempty"`

	// The fully-qualified URIs that point to your data in Google Cloud.
	// For Google Cloud Storage URIs: Each URI can contain one '*' wildcard character
	// and it must come after the 'bucket' name. Size limits related to load jobs apply
	// to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
	// specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
	// For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '*' wildcard character is not allowed.
	SourceUris []*string `json:"sourceUris,omitempty" tf:"source_uris,omitempty"`

	// Time-based partitioning specification for the destination table.
	// Structure is documented below.
	TimePartitioning []TimePartitioningInitParameters `json:"timePartitioning,omitempty" tf:"time_partitioning,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type LoadObservation struct {

	// Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
	// If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
	// an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
	AllowJaggedRows *bool `json:"allowJaggedRows,omitempty" tf:"allow_jagged_rows,omitempty"`

	// Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
	// The default value is false.
	AllowQuotedNewlines *bool `json:"allowQuotedNewlines,omitempty" tf:"allow_quoted_newlines,omitempty"`

	// Indicates if we should automatically infer the options and schema for CSV and JSON sources.
	Autodetect *bool `json:"autodetect,omitempty" tf:"autodetect,omitempty"`

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	DestinationEncryptionConfiguration []LoadDestinationEncryptionConfigurationObservation `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// The destination table to load the data into.
	// Structure is documented below.
	DestinationTable []LoadDestinationTableObservation `json:"destinationTable,omitempty" tf:"destination_table,omitempty"`

	// The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
	// The default value is UTF-8. BigQuery decodes the data after the raw, binary data
	// has been split using the values of the quote and fieldDelimiter properties.
	Encoding *string `json:"encoding,omitempty" tf:"encoding,omitempty"`

	// The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
	// To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
	// the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
	// data in its raw, binary state. BigQuery also supports the escape sequence "\t" to specify a tab separator.
	// The default value is a comma (',').
	FieldDelimiter *string `json:"fieldDelimiter,omitempty" tf:"field_delimiter,omitempty"`

	// Indicates if BigQuery should allow extra values that are not represented in the table schema.
	// If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
	// and if there are too many bad records, an invalid error is returned in the job result.
	// The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
	// CSV: Trailing columns
	// JSON: Named values that don't match any column names
	IgnoreUnknownValues *bool `json:"ignoreUnknownValues,omitempty" tf:"ignore_unknown_values,omitempty"`

	// If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
	// For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
	// GeoJSON: set to GEOJSON.
	JSONExtension *string `json:"jsonExtension,omitempty" tf:"json_extension,omitempty"`

	// The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
	// an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
	MaxBadRecords *float64 `json:"maxBadRecords,omitempty" tf:"max_bad_records,omitempty"`

	// Specifies a string that represents a null value in a CSV file. For example, if you specify "\N", BigQuery interprets "\N" as a null value
	// when loading a CSV file. The default value is the empty string. If you set this property to a custom value, BigQuery throws an error if an
	// empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
	// an empty value.
	NullMarker *string `json:"nullMarker,omitempty" tf:"null_marker,omitempty"`

	// Parquet Options for load and make external tables.
	// Structure is documented below.
	ParquetOptions []ParquetOptionsObservation `json:"parquetOptions,omitempty" tf:"parquet_options,omitempty"`

	// If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
	// Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
	// If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
	ProjectionFields []*string `json:"projectionFields,omitempty" tf:"projection_fields,omitempty"`

	// The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
	// and then uses the first byte of the encoded string to split the data in its raw, binary state.
	// The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
	// If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
	Quote *string `json:"quote,omitempty" tf:"quote,omitempty"`

	// Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
	// supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
	// For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	SchemaUpdateOptions []*string `json:"schemaUpdateOptions,omitempty" tf:"schema_update_options,omitempty"`

	// The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
	// The default value is 0. This property is useful if you have header rows in the file that should be skipped.
	// When autodetect is on, the behavior is the following:
	// skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
	// the row is read as data. Otherwise data is read starting from the second row.
	// skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
	// skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
	// row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
	SkipLeadingRows *float64 `json:"skipLeadingRows,omitempty" tf:"skip_leading_rows,omitempty"`

	// The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
	// For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
	// For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
	// The default value is CSV.
	SourceFormat *string `json:"sourceFormat,omitempty" tf:"source_format,omitempty"`

	// The fully-qualified URIs that point to your data in Google Cloud.
	// For Google Cloud Storage URIs: Each URI can contain one '*' wildcard character
	// and it must come after the 'bucket' name. Size limits related to load jobs apply
	// to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
	// specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
	// For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '*' wildcard character is not allowed.
	SourceUris []*string `json:"sourceUris,omitempty" tf:"source_uris,omitempty"`

	// Time-based partitioning specification for the destination table.
	// Structure is documented below.
	TimePartitioning []TimePartitioningObservation `json:"timePartitioning,omitempty" tf:"time_partitioning,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type LoadParameters struct {

	// Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
	// If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
	// an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
	// +kubebuilder:validation:Optional
	AllowJaggedRows *bool `json:"allowJaggedRows,omitempty" tf:"allow_jagged_rows,omitempty"`

	// Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
	// The default value is false.
	// +kubebuilder:validation:Optional
	AllowQuotedNewlines *bool `json:"allowQuotedNewlines,omitempty" tf:"allow_quoted_newlines,omitempty"`

	// Indicates if we should automatically infer the options and schema for CSV and JSON sources.
	// +kubebuilder:validation:Optional
	Autodetect *bool `json:"autodetect,omitempty" tf:"autodetect,omitempty"`

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	// +kubebuilder:validation:Optional
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	DestinationEncryptionConfiguration []LoadDestinationEncryptionConfigurationParameters `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// The destination table to load the data into.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	DestinationTable []LoadDestinationTableParameters `json:"destinationTable" tf:"destination_table,omitempty"`

	// The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
	// The default value is UTF-8. BigQuery decodes the data after the raw, binary data
	// has been split using the values of the quote and fieldDelimiter properties.
	// +kubebuilder:validation:Optional
	Encoding *string `json:"encoding,omitempty" tf:"encoding,omitempty"`

	// The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
	// To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
	// the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
	// data in its raw, binary state. BigQuery also supports the escape sequence "\t" to specify a tab separator.
	// The default value is a comma (',').
	// +kubebuilder:validation:Optional
	FieldDelimiter *string `json:"fieldDelimiter,omitempty" tf:"field_delimiter,omitempty"`

	// Indicates if BigQuery should allow extra values that are not represented in the table schema.
	// If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
	// and if there are too many bad records, an invalid error is returned in the job result.
	// The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
	// CSV: Trailing columns
	// JSON: Named values that don't match any column names
	// +kubebuilder:validation:Optional
	IgnoreUnknownValues *bool `json:"ignoreUnknownValues,omitempty" tf:"ignore_unknown_values,omitempty"`

	// If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
	// For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
	// GeoJSON: set to GEOJSON.
	// +kubebuilder:validation:Optional
	JSONExtension *string `json:"jsonExtension,omitempty" tf:"json_extension,omitempty"`

	// The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
	// an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
	// +kubebuilder:validation:Optional
	MaxBadRecords *float64 `json:"maxBadRecords,omitempty" tf:"max_bad_records,omitempty"`

	// Specifies a string that represents a null value in a CSV file. For example, if you specify "\N", BigQuery interprets "\N" as a null value
	// when loading a CSV file. The default value is the empty string. If you set this property to a custom value, BigQuery throws an error if an
	// empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
	// an empty value.
	// +kubebuilder:validation:Optional
	NullMarker *string `json:"nullMarker,omitempty" tf:"null_marker,omitempty"`

	// Parquet Options for load and make external tables.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	ParquetOptions []ParquetOptionsParameters `json:"parquetOptions,omitempty" tf:"parquet_options,omitempty"`

	// If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
	// Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
	// If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
	// +kubebuilder:validation:Optional
	ProjectionFields []*string `json:"projectionFields,omitempty" tf:"projection_fields,omitempty"`

	// The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
	// and then uses the first byte of the encoded string to split the data in its raw, binary state.
	// The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
	// If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
	// +kubebuilder:validation:Optional
	Quote *string `json:"quote,omitempty" tf:"quote,omitempty"`

	// Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
	// supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
	// For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	// +kubebuilder:validation:Optional
	SchemaUpdateOptions []*string `json:"schemaUpdateOptions,omitempty" tf:"schema_update_options,omitempty"`

	// The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
	// The default value is 0. This property is useful if you have header rows in the file that should be skipped.
	// When autodetect is on, the behavior is the following:
	// skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
	// the row is read as data. Otherwise data is read starting from the second row.
	// skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
	// skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
	// row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
	// +kubebuilder:validation:Optional
	SkipLeadingRows *float64 `json:"skipLeadingRows,omitempty" tf:"skip_leading_rows,omitempty"`

	// The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
	// For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
	// For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
	// The default value is CSV.
	// +kubebuilder:validation:Optional
	SourceFormat *string `json:"sourceFormat,omitempty" tf:"source_format,omitempty"`

	// The fully-qualified URIs that point to your data in Google Cloud.
	// For Google Cloud Storage URIs: Each URI can contain one '*' wildcard character
	// and it must come after the 'bucket' name. Size limits related to load jobs apply
	// to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
	// specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
	// For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '*' wildcard character is not allowed.
	// +kubebuilder:validation:Optional
	SourceUris []*string `json:"sourceUris" tf:"source_uris,omitempty"`

	// Time-based partitioning specification for the destination table.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	TimePartitioning []TimePartitioningParameters `json:"timePartitioning,omitempty" tf:"time_partitioning,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	// +kubebuilder:validation:Optional
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type ParquetOptionsInitParameters struct {

	// If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
	EnableListInference *bool `json:"enableListInference,omitempty" tf:"enable_list_inference,omitempty"`

	// If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
	EnumAsString *bool `json:"enumAsString,omitempty" tf:"enum_as_string,omitempty"`
}

type ParquetOptionsObservation struct {

	// If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
	EnableListInference *bool `json:"enableListInference,omitempty" tf:"enable_list_inference,omitempty"`

	// If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
	EnumAsString *bool `json:"enumAsString,omitempty" tf:"enum_as_string,omitempty"`
}

type ParquetOptionsParameters struct {

	// If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
	// +kubebuilder:validation:Optional
	EnableListInference *bool `json:"enableListInference,omitempty" tf:"enable_list_inference,omitempty"`

	// If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
	// +kubebuilder:validation:Optional
	EnumAsString *bool `json:"enumAsString,omitempty" tf:"enum_as_string,omitempty"`
}

type QueryDestinationEncryptionConfigurationInitParameters struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`
}

type QueryDestinationEncryptionConfigurationObservation struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`

	// (Output)
	// Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
	KMSKeyVersion *string `json:"kmsKeyVersion,omitempty" tf:"kms_key_version,omitempty"`
}

type QueryDestinationEncryptionConfigurationParameters struct {

	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	// +kubebuilder:validation:Optional
	KMSKeyName *string `json:"kmsKeyName" tf:"kms_key_name,omitempty"`
}

type QueryDestinationTableInitParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`

	// Reference to a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDRef *v1.Reference `json:"tableIdRef,omitempty" tf:"-"`

	// Selector for a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDSelector *v1.Selector `json:"tableIdSelector,omitempty" tf:"-"`
}

type QueryDestinationTableObservation struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`
}

type QueryDestinationTableParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +kubebuilder:validation:Optional
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`

	// Reference to a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDRef *v1.Reference `json:"tableIdRef,omitempty" tf:"-"`

	// Selector for a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDSelector *v1.Selector `json:"tableIdSelector,omitempty" tf:"-"`
}

type QueryInitParameters struct {

	// If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
	// Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
	// However, you must still set destinationTable when result size exceeds the allowed maximum response size.
	AllowLargeResults *bool `json:"allowLargeResults,omitempty" tf:"allow_large_results,omitempty"`

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
	// Structure is documented below.
	DefaultDataset []DefaultDatasetInitParameters `json:"defaultDataset,omitempty" tf:"default_dataset,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	DestinationEncryptionConfiguration []QueryDestinationEncryptionConfigurationInitParameters `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// Describes the table where the query results should be stored.
	// This property must be set for large results that exceed the maximum response size.
	// For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
	// Structure is documented below.
	DestinationTable []QueryDestinationTableInitParameters `json:"destinationTable,omitempty" tf:"destination_table,omitempty"`

	// If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
	// allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
	FlattenResults *bool `json:"flattenResults,omitempty" tf:"flatten_results,omitempty"`

	// Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	MaximumBillingTier *float64 `json:"maximumBillingTier,omitempty" tf:"maximum_billing_tier,omitempty"`

	// Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	MaximumBytesBilled *string `json:"maximumBytesBilled,omitempty" tf:"maximum_bytes_billed,omitempty"`

	// Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
	ParameterMode *string `json:"parameterMode,omitempty" tf:"parameter_mode,omitempty"`

	// Specifies a priority for the query.
	// Default value is INTERACTIVE.
	// Possible values are: INTERACTIVE, BATCH.
	Priority *string `json:"priority,omitempty" tf:"priority,omitempty"`

	// SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
	// NOTE: queries containing DML language
	// (DELETE, UPDATE, MERGE, INSERT) must specify create_disposition = "" and write_disposition = "".
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// Allows the schema of the destination table to be updated as a side effect of the query job.
	// Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
	// specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
	// One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	SchemaUpdateOptions []*string `json:"schemaUpdateOptions,omitempty" tf:"schema_update_options,omitempty"`

	// Options controlling the execution of scripts.
	// Structure is documented below.
	ScriptOptions []ScriptOptionsInitParameters `json:"scriptOptions,omitempty" tf:"script_options,omitempty"`

	// Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
	// If set to false, the query will use BigQuery's standard SQL.
	UseLegacySQL *bool `json:"useLegacySql,omitempty" tf:"use_legacy_sql,omitempty"`

	// Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
	// tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
	// The default value is true.
	UseQueryCache *bool `json:"useQueryCache,omitempty" tf:"use_query_cache,omitempty"`

	// Describes user-defined function resources used in the query.
	// Structure is documented below.
	UserDefinedFunctionResources []UserDefinedFunctionResourcesInitParameters `json:"userDefinedFunctionResources,omitempty" tf:"user_defined_function_resources,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type QueryObservation struct {

	// If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
	// Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
	// However, you must still set destinationTable when result size exceeds the allowed maximum response size.
	AllowLargeResults *bool `json:"allowLargeResults,omitempty" tf:"allow_large_results,omitempty"`

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
	// Structure is documented below.
	DefaultDataset []DefaultDatasetObservation `json:"defaultDataset,omitempty" tf:"default_dataset,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	DestinationEncryptionConfiguration []QueryDestinationEncryptionConfigurationObservation `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// Describes the table where the query results should be stored.
	// This property must be set for large results that exceed the maximum response size.
	// For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
	// Structure is documented below.
	DestinationTable []QueryDestinationTableObservation `json:"destinationTable,omitempty" tf:"destination_table,omitempty"`

	// If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
	// allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
	FlattenResults *bool `json:"flattenResults,omitempty" tf:"flatten_results,omitempty"`

	// Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	MaximumBillingTier *float64 `json:"maximumBillingTier,omitempty" tf:"maximum_billing_tier,omitempty"`

	// Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	MaximumBytesBilled *string `json:"maximumBytesBilled,omitempty" tf:"maximum_bytes_billed,omitempty"`

	// Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
	ParameterMode *string `json:"parameterMode,omitempty" tf:"parameter_mode,omitempty"`

	// Specifies a priority for the query.
	// Default value is INTERACTIVE.
	// Possible values are: INTERACTIVE, BATCH.
	Priority *string `json:"priority,omitempty" tf:"priority,omitempty"`

	// SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
	// NOTE: queries containing DML language
	// (DELETE, UPDATE, MERGE, INSERT) must specify create_disposition = "" and write_disposition = "".
	Query *string `json:"query,omitempty" tf:"query,omitempty"`

	// Allows the schema of the destination table to be updated as a side effect of the query job.
	// Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
	// specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
	// One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	SchemaUpdateOptions []*string `json:"schemaUpdateOptions,omitempty" tf:"schema_update_options,omitempty"`

	// Options controlling the execution of scripts.
	// Structure is documented below.
	ScriptOptions []ScriptOptionsObservation `json:"scriptOptions,omitempty" tf:"script_options,omitempty"`

	// Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
	// If set to false, the query will use BigQuery's standard SQL.
	UseLegacySQL *bool `json:"useLegacySql,omitempty" tf:"use_legacy_sql,omitempty"`

	// Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
	// tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
	// The default value is true.
	UseQueryCache *bool `json:"useQueryCache,omitempty" tf:"use_query_cache,omitempty"`

	// Describes user-defined function resources used in the query.
	// Structure is documented below.
	UserDefinedFunctionResources []UserDefinedFunctionResourcesObservation `json:"userDefinedFunctionResources,omitempty" tf:"user_defined_function_resources,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type QueryParameters struct {

	// If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
	// Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
	// However, you must still set destinationTable when result size exceeds the allowed maximum response size.
	// +kubebuilder:validation:Optional
	AllowLargeResults *bool `json:"allowLargeResults,omitempty" tf:"allow_large_results,omitempty"`

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	// +kubebuilder:validation:Optional
	CreateDisposition *string `json:"createDisposition,omitempty" tf:"create_disposition,omitempty"`

	// Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	DefaultDataset []DefaultDatasetParameters `json:"defaultDataset,omitempty" tf:"default_dataset,omitempty"`

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	DestinationEncryptionConfiguration []QueryDestinationEncryptionConfigurationParameters `json:"destinationEncryptionConfiguration,omitempty" tf:"destination_encryption_configuration,omitempty"`

	// Describes the table where the query results should be stored.
	// This property must be set for large results that exceed the maximum response size.
	// For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	DestinationTable []QueryDestinationTableParameters `json:"destinationTable,omitempty" tf:"destination_table,omitempty"`

	// If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
	// allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
	// +kubebuilder:validation:Optional
	FlattenResults *bool `json:"flattenResults,omitempty" tf:"flatten_results,omitempty"`

	// Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	// +kubebuilder:validation:Optional
	MaximumBillingTier *float64 `json:"maximumBillingTier,omitempty" tf:"maximum_billing_tier,omitempty"`

	// Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	// +kubebuilder:validation:Optional
	MaximumBytesBilled *string `json:"maximumBytesBilled,omitempty" tf:"maximum_bytes_billed,omitempty"`

	// Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
	// +kubebuilder:validation:Optional
	ParameterMode *string `json:"parameterMode,omitempty" tf:"parameter_mode,omitempty"`

	// Specifies a priority for the query.
	// Default value is INTERACTIVE.
	// Possible values are: INTERACTIVE, BATCH.
	// +kubebuilder:validation:Optional
	Priority *string `json:"priority,omitempty" tf:"priority,omitempty"`

	// SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
	// NOTE: queries containing DML language
	// (DELETE, UPDATE, MERGE, INSERT) must specify create_disposition = "" and write_disposition = "".
	// +kubebuilder:validation:Optional
	Query *string `json:"query" tf:"query,omitempty"`

	// Allows the schema of the destination table to be updated as a side effect of the query job.
	// Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
	// specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
	// One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	// +kubebuilder:validation:Optional
	SchemaUpdateOptions []*string `json:"schemaUpdateOptions,omitempty" tf:"schema_update_options,omitempty"`

	// Options controlling the execution of scripts.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	ScriptOptions []ScriptOptionsParameters `json:"scriptOptions,omitempty" tf:"script_options,omitempty"`

	// Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
	// If set to false, the query will use BigQuery's standard SQL.
	// +kubebuilder:validation:Optional
	UseLegacySQL *bool `json:"useLegacySql,omitempty" tf:"use_legacy_sql,omitempty"`

	// Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
	// tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
	// The default value is true.
	// +kubebuilder:validation:Optional
	UseQueryCache *bool `json:"useQueryCache,omitempty" tf:"use_query_cache,omitempty"`

	// Describes user-defined function resources used in the query.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	UserDefinedFunctionResources []UserDefinedFunctionResourcesParameters `json:"userDefinedFunctionResources,omitempty" tf:"user_defined_function_resources,omitempty"`

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	// +kubebuilder:validation:Optional
	WriteDisposition *string `json:"writeDisposition,omitempty" tf:"write_disposition,omitempty"`
}

type ScriptOptionsInitParameters struct {

	// Determines which statement in the script represents the "key result",
	// used to populate the schema and query results of the script job.
	// Possible values are: LAST, FIRST_SELECT.
	KeyResultStatement *string `json:"keyResultStatement,omitempty" tf:"key_result_statement,omitempty"`

	// Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
	StatementByteBudget *string `json:"statementByteBudget,omitempty" tf:"statement_byte_budget,omitempty"`

	// Timeout period for each statement in a script.
	StatementTimeoutMs *string `json:"statementTimeoutMs,omitempty" tf:"statement_timeout_ms,omitempty"`
}

type ScriptOptionsObservation struct {

	// Determines which statement in the script represents the "key result",
	// used to populate the schema and query results of the script job.
	// Possible values are: LAST, FIRST_SELECT.
	KeyResultStatement *string `json:"keyResultStatement,omitempty" tf:"key_result_statement,omitempty"`

	// Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
	StatementByteBudget *string `json:"statementByteBudget,omitempty" tf:"statement_byte_budget,omitempty"`

	// Timeout period for each statement in a script.
	StatementTimeoutMs *string `json:"statementTimeoutMs,omitempty" tf:"statement_timeout_ms,omitempty"`
}

type ScriptOptionsParameters struct {

	// Determines which statement in the script represents the "key result",
	// used to populate the schema and query results of the script job.
	// Possible values are: LAST, FIRST_SELECT.
	// +kubebuilder:validation:Optional
	KeyResultStatement *string `json:"keyResultStatement,omitempty" tf:"key_result_statement,omitempty"`

	// Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
	// +kubebuilder:validation:Optional
	StatementByteBudget *string `json:"statementByteBudget,omitempty" tf:"statement_byte_budget,omitempty"`

	// Timeout period for each statement in a script.
	// +kubebuilder:validation:Optional
	StatementTimeoutMs *string `json:"statementTimeoutMs,omitempty" tf:"statement_timeout_ms,omitempty"`
}

type SourceModelInitParameters struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the model.
	ModelID *string `json:"modelId,omitempty" tf:"model_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`
}

type SourceModelObservation struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the model.
	ModelID *string `json:"modelId,omitempty" tf:"model_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`
}

type SourceModelParameters struct {

	// The ID of the dataset containing this table.
	// +kubebuilder:validation:Optional
	DatasetID *string `json:"datasetId" tf:"dataset_id,omitempty"`

	// The ID of the model.
	// +kubebuilder:validation:Optional
	ModelID *string `json:"modelId" tf:"model_id,omitempty"`

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId" tf:"project_id,omitempty"`
}

type SourceTableInitParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`

	// Reference to a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDRef *v1.Reference `json:"tableIdRef,omitempty" tf:"-"`

	// Selector for a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDSelector *v1.Selector `json:"tableIdSelector,omitempty" tf:"-"`
}

type SourceTableObservation struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`
}

type SourceTableParameters struct {

	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +kubebuilder:validation:Optional
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// Reference to a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDRef *v1.Reference `json:"datasetIdRef,omitempty" tf:"-"`

	// Selector for a Dataset in bigquery to populate datasetId.
	// +kubebuilder:validation:Optional
	DatasetIDSelector *v1.Selector `json:"datasetIdSelector,omitempty" tf:"-"`

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/crossplane/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`

	// Reference to a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDRef *v1.Reference `json:"tableIdRef,omitempty" tf:"-"`

	// Selector for a Table in bigquery to populate tableId.
	// +kubebuilder:validation:Optional
	TableIDSelector *v1.Selector `json:"tableIdSelector,omitempty" tf:"-"`
}

type SourceTablesInitParameters struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`
}

type SourceTablesObservation struct {

	// The ID of the dataset containing this table.
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the project containing this table.
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	TableID *string `json:"tableId,omitempty" tf:"table_id,omitempty"`
}

type SourceTablesParameters struct {

	// The ID of the dataset containing this table.
	// +kubebuilder:validation:Optional
	DatasetID *string `json:"datasetId,omitempty" tf:"dataset_id,omitempty"`

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	ProjectID *string `json:"projectId,omitempty" tf:"project_id,omitempty"`

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +kubebuilder:validation:Optional
	TableID *string `json:"tableId" tf:"table_id,omitempty"`
}

type StatusInitParameters struct {
}

type StatusObservation struct {

	// (Output)
	// Final error result of the job. If present, indicates that the job has completed and was unsuccessful.
	// Structure is documented below.
	ErrorResult []ErrorResultObservation `json:"errorResult,omitempty" tf:"error_result,omitempty"`

	// (Output)
	// The first errors encountered during the running of the job. The final message
	// includes the number of errors that caused the process to stop. Errors here do
	// not necessarily mean that the job has not completed or was unsuccessful.
	// Structure is documented below.
	Errors []ErrorsObservation `json:"errors,omitempty" tf:"errors,omitempty"`

	// (Output)
	// Running state of the job. Valid states include 'PENDING', 'RUNNING', and 'DONE'.
	State *string `json:"state,omitempty" tf:"state,omitempty"`
}

type StatusParameters struct {
}

type TimePartitioningInitParameters struct {

	// Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
	ExpirationMs *string `json:"expirationMs,omitempty" tf:"expiration_ms,omitempty"`

	// If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
	// The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
	// A wrapper is used here because an empty string is an invalid value.
	Field *string `json:"field,omitempty" tf:"field,omitempty"`

	// The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
	// but in OnePlatform the field will be treated as unset.
	Type *string `json:"type,omitempty" tf:"type,omitempty"`
}

type TimePartitioningObservation struct {

	// Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
	ExpirationMs *string `json:"expirationMs,omitempty" tf:"expiration_ms,omitempty"`

	// If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
	// The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
	// A wrapper is used here because an empty string is an invalid value.
	Field *string `json:"field,omitempty" tf:"field,omitempty"`

	// The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
	// but in OnePlatform the field will be treated as unset.
	Type *string `json:"type,omitempty" tf:"type,omitempty"`
}

type TimePartitioningParameters struct {

	// Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
	// +kubebuilder:validation:Optional
	ExpirationMs *string `json:"expirationMs,omitempty" tf:"expiration_ms,omitempty"`

	// If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
	// The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
	// A wrapper is used here because an empty string is an invalid value.
	// +kubebuilder:validation:Optional
	Field *string `json:"field,omitempty" tf:"field,omitempty"`

	// The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
	// but in OnePlatform the field will be treated as unset.
	// +kubebuilder:validation:Optional
	Type *string `json:"type" tf:"type,omitempty"`
}

type UserDefinedFunctionResourcesInitParameters struct {

	// An inline resource that contains code for a user-defined function (UDF).
	// Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
	InlineCode *string `json:"inlineCode,omitempty" tf:"inline_code,omitempty"`

	// A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
	ResourceURI *string `json:"resourceUri,omitempty" tf:"resource_uri,omitempty"`
}

type UserDefinedFunctionResourcesObservation struct {

	// An inline resource that contains code for a user-defined function (UDF).
	// Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
	InlineCode *string `json:"inlineCode,omitempty" tf:"inline_code,omitempty"`

	// A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
	ResourceURI *string `json:"resourceUri,omitempty" tf:"resource_uri,omitempty"`
}

type UserDefinedFunctionResourcesParameters struct {

	// An inline resource that contains code for a user-defined function (UDF).
	// Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
	// +kubebuilder:validation:Optional
	InlineCode *string `json:"inlineCode,omitempty" tf:"inline_code,omitempty"`

	// A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
	// +kubebuilder:validation:Optional
	ResourceURI *string `json:"resourceUri,omitempty" tf:"resource_uri,omitempty"`
}

// JobSpec defines the desired state of Job
type JobSpec struct {
	v1.ResourceSpec `json:",inline"`
	ForProvider     JobParameters `json:"forProvider"`
	// THIS IS A BETA FIELD. It will be honored
	// unless the Management Policies feature flag is disabled.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	InitProvider JobInitParameters `json:"initProvider,omitempty"`
}

// JobStatus defines the observed state of Job.
type JobStatus struct {
	v1.ResourceStatus `json:",inline"`
	AtProvider        JobObservation `json:"atProvider,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:storageversion

// Job is the Schema for the Jobs API. Jobs are actions that BigQuery runs on your behalf to load data, export data, query data, or copy data.
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,gcp}
type Job struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.jobId) || (has(self.initProvider) && has(self.initProvider.jobId))",message="spec.forProvider.jobId is a required parameter"
	Spec   JobSpec   `json:"spec"`
	Status JobStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// JobList contains a list of Jobs
type JobList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Job `json:"items"`
}

// Repository type metadata.
var (
	Job_Kind             = "Job"
	Job_GroupKind        = schema.GroupKind{Group: CRDGroup, Kind: Job_Kind}.String()
	Job_KindAPIVersion   = Job_Kind + "." + CRDGroupVersion.String()
	Job_GroupVersionKind = CRDGroupVersion.WithKind(Job_Kind)
)

func init() {
	SchemeBuilder.Register(&Job{}, &JobList{})
}
